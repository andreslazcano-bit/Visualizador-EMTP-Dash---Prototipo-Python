# üåê AN√ÅLISIS DE CONECTIVIDAD INSTITUCIONAL
**Visualizador EMTP v2.0 - Capacidad de Integraci√≥n con Sistemas Empresariales**

**Fecha:** 20 de octubre de 2025  
**Consulta:** ¬øPuede la aplicaci√≥n conectarse a servidores institucionales y bases de datos?

---

## üìä RESPUESTA CORTA: **S√ç ‚úÖ**

Tu aplicaci√≥n **ya est√° preparada arquitect√≥nicamente** para conectarse a m√∫ltiples sistemas institucionales. Solo requiere configuraci√≥n, no cambios de c√≥digo significativos.

---

## üèóÔ∏è ARQUITECTURA ACTUAL

### Infraestructura Existente ‚úÖ

Tu app tiene una **arquitectura modular** dise√±ada para m√∫ltiples fuentes de datos:

```python
# config/database.py - YA IMPLEMENTADO
class DatabaseManager:
    - SQL Server support (pyodbc + SQLAlchemy)
    - PostgreSQL support (SQLAlchemy)
    - Connection pooling
    - Error handling robusto
    - Logging detallado
```

```python
# config/settings.py - YA CONFIGURADO
Soporta:
‚úÖ SQL Server (Azure SQL, SQL Server on-premise)
‚úÖ PostgreSQL (AWS RDS, Azure PostgreSQL, local)
‚úÖ MySQL (mencionado en .env.example)
‚úÖ SharePoint (Office 365, SharePoint Server)
‚úÖ Redis (caching distribuido)
‚úÖ Archivos locales (CSV, Parquet, Excel)
```

---

## üîå SISTEMAS SOPORTADOS

### 1. **SQL Server / Azure SQL Database** ‚úÖ LISTO

**Estado:** C√≥digo implementado, solo requiere configuraci√≥n

**Casos de uso t√≠picos:**
- Bases de datos institucionales del Ministerio
- SQL Server on-premise corporativo
- Azure SQL Database en la nube
- Datawarehouse centralizado

**Configuraci√≥n requerida (.env):**
```bash
SQL_SERVER_ENABLED=True
SQL_SERVER_HOST=servidor.mineduc.cl  # o IP institucional
SQL_SERVER_PORT=1433
SQL_SERVER_DATABASE=EMTP_Produccion
SQL_SERVER_USERNAME=app_emtp_readonly
SQL_SERVER_PASSWORD=<password_segura>
SQL_SERVER_DRIVER=ODBC Driver 17 for SQL Server
```

**Requisitos:**
```bash
# Instalar driver ODBC (una sola vez en el servidor)
# macOS:
brew install unixodbc
brew tap microsoft/mssql-release
brew install msodbcsql17

# Linux (Ubuntu/Debian):
curl https://packages.microsoft.com/keys/microsoft.asc | apt-key add -
curl https://packages.microsoft.com/config/ubuntu/20.04/prod.list > /etc/apt/sources.list.d/mssql-release.list
apt-get update
apt-get install msodbcsql17 unixodbc-dev

# Windows: Ya incluido generalmente
```

**Ventajas:**
- ‚úÖ Perfecto para grandes vol√∫menes de datos
- ‚úÖ Queries complejas con JOINs
- ‚úÖ Stored procedures
- ‚úÖ Transacciones
- ‚úÖ Alta disponibilidad con Always On

**Limitaciones:**
- ‚ö†Ô∏è Requiere licencia SQL Server (o Azure SQL)
- ‚ö†Ô∏è Puede requerir VPN si est√° on-premise

---

### 2. **PostgreSQL** ‚úÖ LISTO

**Estado:** C√≥digo implementado, solo requiere configuraci√≥n

**Casos de uso t√≠picos:**
- Bases de datos open-source institucionales
- AWS RDS PostgreSQL
- Google Cloud SQL PostgreSQL
- Azure Database for PostgreSQL

**Configuraci√≥n requerida (.env):**
```bash
POSTGRES_ENABLED=True
POSTGRES_HOST=postgres.mineduc.cl
POSTGRES_PORT=5432
POSTGRES_DATABASE=emtp_data
POSTGRES_USERNAME=app_reader
POSTGRES_PASSWORD=<password_segura>
```

**No requiere drivers adicionales** (psycopg2 ya en requirements.txt)

**Ventajas:**
- ‚úÖ Open source (sin costos de licencia)
- ‚úÖ JSON support nativo (para datos semi-estructurados)
- ‚úÖ Full-text search
- ‚úÖ PostGIS para datos geogr√°ficos
- ‚úÖ Excelente para an√°lisis

---

### 3. **MySQL / MariaDB** ‚ö†Ô∏è F√ÅCIL DE AGREGAR

**Estado:** Mencionado en .env pero falta c√≥digo

**Tiempo de implementaci√≥n:** 30 minutos

**C√≥digo a agregar:**
```python
# En config/database.py
def get_mysql_engine(self) -> Optional[Engine]:
    """Obtiene engine para MySQL"""
    if not settings.MYSQL_ENABLED:
        return None
    
    connection_string = (
        f"mysql+pymysql://{settings.MYSQL_USERNAME}:"
        f"{settings.MYSQL_PASSWORD}@{settings.MYSQL_HOST}:"
        f"{settings.MYSQL_PORT}/{settings.MYSQL_DATABASE}"
    )
    
    self._engines['mysql'] = create_engine(
        connection_string,
        poolclass=NullPool
    )
    return self._engines['mysql']
```

**Dependencia adicional:**
```bash
pip install pymysql
```

---

### 4. **SharePoint / Office 365** ‚úÖ CONFIGURADO

**Estado:** Variables de entorno listas, falta implementaci√≥n

**Casos de uso t√≠picos:**
- Archivos Excel en SharePoint del Ministerio
- Documentos colaborativos
- Listas de SharePoint como fuente de datos
- Integraci√≥n con Office 365

**Tiempo de implementaci√≥n:** 2-3 horas

**Librer√≠a recomendada:**
```bash
pip install Office365-REST-Python-Client
```

**Configuraci√≥n (.env):**
```bash
SHAREPOINT_ENABLED=True
SHAREPOINT_SITE_URL=https://mineduc.sharepoint.com/sites/EMTP
SHAREPOINT_CLIENT_ID=<azure_app_id>
SHAREPOINT_CLIENT_SECRET=<azure_app_secret>
# O autenticaci√≥n de usuario:
SHAREPOINT_USERNAME=usuario@mineduc.cl
SHAREPOINT_PASSWORD=<password>
```

**Ejemplo de c√≥digo:**
```python
from office365.sharepoint.client_context import ClientContext
from office365.runtime.auth.client_credential import ClientCredential

def get_sharepoint_file(file_path: str) -> pd.DataFrame:
    """Descarga archivo Excel desde SharePoint"""
    
    credentials = ClientCredential(
        settings.SHAREPOINT_CLIENT_ID,
        settings.SHAREPOINT_CLIENT_SECRET
    )
    
    ctx = ClientContext(settings.SHAREPOINT_SITE_URL).with_credentials(credentials)
    
    # Descargar archivo
    response = ctx.web.get_file_by_server_relative_url(file_path).download()
    
    # Leer en pandas
    return pd.read_excel(response.content)
```

---

### 5. **Oracle Database** ‚ö†Ô∏è REQUIERE IMPLEMENTACI√ìN

**Estado:** No implementado pero factible

**Tiempo de implementaci√≥n:** 1-2 horas

**Casos de uso:**
- Sistemas legacy institucionales
- ERP corporativos (SAP, Oracle EBS)
- Datawarehouse Oracle

**Implementaci√≥n:**
```bash
# Instalar driver Oracle Instant Client
# + librer√≠a cx_Oracle
pip install cx_Oracle
```

```python
# Agregar a database.py
def get_oracle_engine(self) -> Optional[Engine]:
    connection_string = (
        f"oracle+cx_oracle://{username}:{password}@"
        f"{host}:{port}/?service_name={service_name}"
    )
    return create_engine(connection_string)
```

---

### 6. **APIs REST / Web Services** ‚úÖ COMPATIBLE

**Estado:** F√°cilmente integrable

**Casos de uso:**
- APIs del Ministerio de Educaci√≥n
- MINEDUC API
- DEMRE (PSU/PAES)
- Otros servicios gubernamentales

**Implementaci√≥n t√≠pica:**
```python
import requests

def get_data_from_api(endpoint: str) -> pd.DataFrame:
    """Obtiene datos desde API institucional"""
    
    # Autenticaci√≥n
    headers = {
        'Authorization': f'Bearer {settings.API_TOKEN}',
        'Content-Type': 'application/json'
    }
    
    # Request
    response = requests.get(
        f"{settings.API_BASE_URL}/{endpoint}",
        headers=headers,
        timeout=30
    )
    
    # Convertir a DataFrame
    data = response.json()
    return pd.DataFrame(data)
```

**Agregar a .env:**
```bash
API_ENABLED=True
API_BASE_URL=https://api.mineduc.cl/v1
API_TOKEN=<token_institucional>
```

---

### 7. **Azure Data Lake / AWS S3** ‚ö†Ô∏è F√ÅCIL DE AGREGAR

**Estado:** No implementado pero trivial

**Tiempo:** 1 hora

**Casos de uso:**
- Data lakes institucionales
- Almacenamiento masivo de archivos
- Backup en la nube
- Archivos Parquet grandes

**Azure:**
```bash
pip install azure-storage-blob azure-identity
```

```python
from azure.storage.blob import BlobServiceClient
from azure.identity import DefaultAzureCredential

def read_from_azure_blob(container: str, blob_name: str) -> pd.DataFrame:
    """Lee archivo desde Azure Blob Storage"""
    
    credential = DefaultAzureCredential()
    blob_service = BlobServiceClient(
        account_url=settings.AZURE_STORAGE_URL,
        credential=credential
    )
    
    blob_client = blob_service.get_blob_client(
        container=container,
        blob=blob_name
    )
    
    # Descargar y leer
    blob_data = blob_client.download_blob().readall()
    return pd.read_parquet(io.BytesIO(blob_data))
```

**AWS S3:**
```bash
pip install boto3
```

```python
import boto3
import io

def read_from_s3(bucket: str, key: str) -> pd.DataFrame:
    """Lee archivo desde S3"""
    
    s3 = boto3.client('s3',
        aws_access_key_id=settings.AWS_ACCESS_KEY,
        aws_secret_access_key=settings.AWS_SECRET_KEY
    )
    
    obj = s3.get_object(Bucket=bucket, Key=key)
    return pd.read_parquet(io.BytesIO(obj['Body'].read()))
```

---

## üîê CONSIDERACIONES DE SEGURIDAD

### Networking

**VPN / Red Privada:**
```
Si los servidores est√°n en red interna institucional:

‚úÖ Opci√≥n 1: Desplegar app en servidor interno
‚úÖ Opci√≥n 2: VPN institucional (OpenVPN, Cisco AnyConnect)
‚úÖ Opci√≥n 3: Azure ExpressRoute / AWS Direct Connect
‚úÖ Opci√≥n 4: SSH Tunnel para desarrollo
```

**Firewall:**
```bash
# Servidor debe permitir conexiones salientes a:
- Puerto 1433 (SQL Server)
- Puerto 5432 (PostgreSQL)
- Puerto 3306 (MySQL)
- Puerto 443 (HTTPS para APIs)
```

### Autenticaci√≥n

**Mejores pr√°cticas:**
```python
‚úÖ Credenciales en .env (NUNCA en c√≥digo)
‚úÖ Usuarios de solo lectura (read-only)
‚úÖ Principio de m√≠nimo privilegio
‚úÖ Rotaci√≥n peri√≥dica de passwords
‚úÖ Azure Managed Identity (si en Azure)
‚úÖ AWS IAM Roles (si en AWS)
```

**Ejemplo de usuario read-only SQL Server:**
```sql
-- En el servidor institucional
CREATE LOGIN app_emtp_readonly WITH PASSWORD = 'StrongPassword123!';
CREATE USER app_emtp_readonly FOR LOGIN app_emtp_readonly;

-- Solo SELECT en tablas necesarias
GRANT SELECT ON dbo.Matricula TO app_emtp_readonly;
GRANT SELECT ON dbo.Docentes TO app_emtp_readonly;
GRANT SELECT ON dbo.Establecimientos TO app_emtp_readonly;
-- etc.

-- NO DAR:
-- DENY INSERT, UPDATE, DELETE
```

### Encriptaci√≥n

**En tr√°nsito:**
```python
# SQL Server con SSL/TLS
connection_string += ";Encrypt=yes;TrustServerCertificate=no"

# PostgreSQL con SSL
connection_string += "?sslmode=require"

# APIs siempre HTTPS
API_BASE_URL = "https://..."  # Nunca http://
```

**En reposo:**
```bash
# Archivos .env con permisos restrictivos
chmod 600 .env

# O usar vault:
- Azure Key Vault
- AWS Secrets Manager
- HashiCorp Vault
```

---

## üöÄ PLAN DE IMPLEMENTACI√ìN

### Fase 1: Validaci√≥n (1-2 d√≠as)

**Objetivo:** Probar conectividad b√°sica

```bash
# 1. Obtener credenciales de prueba del equipo IT institucional
# 2. Configurar .env con datos de desarrollo/test
# 3. Instalar drivers necesarios (ODBC, etc.)
# 4. Ejecutar test de conexi√≥n
```

**Test b√°sico:**
```python
# Crear scripts/test_db_connection.py
from config.database import DatabaseManager

db = DatabaseManager()

# Test SQL Server
engine = db.get_sql_server_engine()
if engine:
    result = db.execute_query("SELECT TOP 10 * FROM Matricula")
    print(f"‚úÖ SQL Server OK: {len(result)} registros")
else:
    print("‚ùå SQL Server failed")

# Test PostgreSQL
engine = db.get_postgres_engine()
if engine:
    result = db.execute_query("SELECT * FROM matricula LIMIT 10", engine_name='postgres')
    print(f"‚úÖ PostgreSQL OK: {len(result)} registros")
else:
    print("‚ùå PostgreSQL failed")
```

### Fase 2: Migraci√≥n de Datos (2-5 d√≠as)

**Objetivo:** Reemplazar datos simulados con datos reales

```python
# Modificar src/data/loaders.py

def load_matricula_data() -> pd.DataFrame:
    """Carga datos de matr√≠cula desde fuente configurada"""
    
    if settings.SQL_SERVER_ENABLED:
        # Desde SQL Server institucional
        db = DatabaseManager()
        query = """
            SELECT 
                anio, 
                codigo_region,
                codigo_establecimiento,
                nivel,
                modalidad,
                COUNT(*) as matricula
            FROM MatriculaOficial
            WHERE anio >= 2019
            GROUP BY anio, codigo_region, codigo_establecimiento, nivel, modalidad
        """
        return db.execute_query(query)
    
    elif settings.POSTGRES_ENABLED:
        # Desde PostgreSQL
        db = DatabaseManager()
        return db.execute_query(
            "SELECT * FROM matricula_consolidada",
            engine_name='postgres'
        )
    
    else:
        # Fallback a datos locales
        return pd.read_parquet('data/processed/matricula.parquet')
```

### Fase 3: Optimizaci√≥n (3-7 d√≠as)

**Objetivo:** Performance y caching

```python
# Implementar caching con Redis
import redis
import pickle

def get_cached_data(key: str, query_func, ttl: int = 3600):
    """Cache con Redis"""
    
    if not settings.REDIS_ENABLED:
        return query_func()
    
    r = redis.Redis(
        host=settings.REDIS_HOST,
        port=settings.REDIS_PORT,
        password=settings.REDIS_PASSWORD
    )
    
    # Buscar en cache
    cached = r.get(key)
    if cached:
        logger.info(f"‚úÖ Cache hit: {key}")
        return pickle.loads(cached)
    
    # Query + cache
    data = query_func()
    r.setex(key, ttl, pickle.dumps(data))
    logger.info(f"üìù Cache miss: {key} - Guardado por {ttl}s")
    
    return data
```

### Fase 4: Producci√≥n (1 semana)

**Checklist:**
```markdown
[ ] Credenciales de producci√≥n configuradas
[ ] Usuario read-only con permisos m√≠nimos
[ ] SSL/TLS habilitado en todas las conexiones
[ ] Firewall rules configurados
[ ] VPN configurada (si es necesario)
[ ] Monitoring y alertas
[ ] Backup de configuraci√≥n
[ ] Documentaci√≥n de DR (Disaster Recovery)
[ ] Logs de acceso a datos habilitados
[ ] Testing de carga completado
```

---

## üìà ESTIMACI√ìN DE ESFUERZO

| Tarea | Tiempo | Dificultad | Dependencias |
|-------|--------|------------|--------------|
| Test de conectividad SQL Server | 2-4 horas | Baja | Credenciales IT |
| Test de conectividad PostgreSQL | 1-2 horas | Baja | Credenciales IT |
| Migraci√≥n de datos | 1-2 d√≠as | Media | Queries aprobadas |
| Implementar caching (Redis) | 1 d√≠a | Media | Servidor Redis |
| SharePoint integration | 2-3 d√≠as | Media-Alta | App registration |
| Optimizaci√≥n de queries | 2-3 d√≠as | Media | DBA review |
| Testing completo | 2-3 d√≠as | Media | Datos de prueba |
| Documentaci√≥n | 1 d√≠a | Baja | - |
| **TOTAL** | **2-3 semanas** | - | - |

---

## ‚úÖ RECOMENDACIONES

### Corto Plazo (Pr√≥xima semana)

1. **Solicitar credenciales de prueba al equipo IT**
   - Usuario read-only en base de datos de desarrollo
   - VPN si es necesario
   - Documentaci√≥n de esquema de base de datos

2. **Ejecutar test de conexi√≥n**
   - Validar que puedes conectarte desde tu m√°quina
   - Probar queries b√°sicas
   - Medir latencia y performance

3. **Mapear esquema de datos**
   - Documentar tablas disponibles
   - Identificar campos clave
   - Validar calidad de datos

### Medio Plazo (Pr√≥ximo mes)

4. **Implementar capa de abstracci√≥n**
   - DataLoader que funcione con m√∫ltiples fuentes
   - Fallback autom√°tico a datos locales
   - Cache inteligente

5. **Optimizar queries**
   - √çndices apropiados
   - Queries parametrizadas
   - Paginaci√≥n para grandes datasets

6. **Monitoreo**
   - Log de tiempos de query
   - Alertas si conexi√≥n falla
   - Dashboard de health

### Largo Plazo (Pr√≥ximos 3 meses)

7. **Alta disponibilidad**
   - Failover autom√°tico
   - M√∫ltiples r√©plicas read-only
   - Circuit breaker pattern

8. **Seguridad avanzada**
   - Auditor√≠a de accesos
   - Encriptaci√≥n de datos sensibles
   - Rotaci√≥n autom√°tica de credenciales

9. **Escalabilidad**
   - Connection pooling avanzado
   - Distributed caching
   - Microservicios si es necesario

---

## üéØ CONCLUSI√ìN

### ‚úÖ TU APP EST√Å LISTA

Tu aplicaci√≥n tiene una **arquitectura s√≥lida y preparada** para conectarse a sistemas institucionales:

1. ‚úÖ **SQL Server:** Implementado y listo
2. ‚úÖ **PostgreSQL:** Implementado y listo
3. ‚ö†Ô∏è **MySQL:** F√°cil de agregar (30 min)
4. ‚ö†Ô∏è **SharePoint:** Requiere implementaci√≥n (2-3 horas)
5. ‚ö†Ô∏è **APIs REST:** Muy f√°cil (1 hora)
6. ‚ö†Ô∏è **Cloud Storage:** F√°cil de agregar (1-2 horas)

### üöÄ PR√ìXIMO PASO

**Recomendaci√≥n:** Empezar con una **prueba de concepto (PoC)** de 1 semana:

```
D√≠a 1-2: Solicitar credenciales de prueba + test de conectividad
D√≠a 3-4: Implementar carga de 1 tabla real (ej: Matricula)
D√≠a 5: Validar resultados con datos de producci√≥n vs simulados
```

Si el PoC es exitoso, tienes **confianza t√©cnica** para planificar la migraci√≥n completa.

### üí° VENTAJA COMPETITIVA

El hecho de que **ya tengas esta arquitectura implementada** es una gran ventaja. Muchas apps no planean para m√∫ltiples fuentes de datos desde el inicio y terminan con c√≥digo muy acoplado.

Tu app puede:
- ‚úÖ Conectarse a bases de datos institucionales
- ‚úÖ Leer de archivos locales como fallback
- ‚úÖ Integrar con SharePoint/Office 365
- ‚úÖ Consumir APIs gubernamentales
- ‚úÖ Escalar a cloud (Azure, AWS)

---

**Respuesta final:** **S√ç, tu app puede conectarse a servidores institucionales.** Solo necesitas configuraci√≥n y credenciales, el c√≥digo ya est√° preparado. üéâ

